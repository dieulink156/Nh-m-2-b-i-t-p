CustomerID: Mã số của khách hàng.
Churn: Trạng thái churn của khách hàng, có thể là "Yes" (rời bỏ) hoặc "No" (không rời bỏ).
MonthlyRevenue: Doanh thu hàng tháng của khách hàng.
MonthlyMinutes: Số phút sử dụng hàng tháng của khách hàng.
TotalRecurringCharge: Tổng lệ phí định kỳ hàng tháng.
DirectorAssistedCalls: Số cuộc gọi được hỗ trợ bởi giám đốc.
OverageMinutes: Số phút vượt giới hạn hàng tháng.
RoamingCalls: Số cuộc gọi lưu động.
PercChangeMinutes: Tỷ lệ thay đổi phút sử dụng so với tháng trước.
PercChangeRevenues: Tỷ lệ thay đổi doanh thu so với tháng trước.
DroppedCalls: Số cuộc gọi bị từ chối hoặc kết thúc đột ngột.
BlockedCalls: Số cuộc gọi bị chặn.
UnansweredCalls: Số cuộc gọi không được trả lời.
CustomerCareCalls: Số cuộc gọi tới dịch vụ chăm sóc khách hàng.
ThreewayCalls: Số cuộc gọi ba chiều.
ReceivedCalls: Số cuộc gọi đến.
OutboundCalls: Số cuộc gọi đi.
InboundCalls: Số cuộc gọi đến.
PeakCallsInOut: Số cuộc gọi đỉnh hàng ngày (đến và đi).
OffPeakCallsInOut: Số cuộc gọi ngoại giờ hàng ngày (đến và đi).
DroppedBlockedCalls: Số cuộc gọi bị từ chối hoặc chặn.
CallForwardingCalls: Số cuộc gọi chuyển tiếp.
CallWaitingCalls: Số cuộc gọi đang chờ.
MonthsInService: Số tháng sử dụng dịch vụ.
UniqueSubs: Số lượng thuê bao duy nhất.
ActiveSubs: Số lượng thuê bao hoạt động.
ServiceArea: Khu vực dịch vụ.
và các thuộc tính khác như Handsets, IncomeGroup, CreditRating, Occupation, MaritalStatus, vv.
# Delete the rows with 'NA' values
for column in df.columns:
    if df.schema[column].dataType == pyspark.sql.types.StringType():
        df = df.filter(df[column] != "NA")


# Check the description of Churn column
df.select("Churn").describe().show()


# Going through all columns required to be transfer into numerical values
columns_string = [
    column
    for column in df.columns
    if df.schema[column].dataType == pyspark.sql.types.StringType()
]
columns_string


# create the view of df under spark
df.createOrReplaceTempView("df")

# check the distribution of picked columns
spark.sql(
    "SELECT \
            Churn, \
            CAST (avg(MonthlyRevenue) as decimal(8,2)) as avg_MonthlyRevenue, \
            CAST (avg(MonthlyMinutes) as decimal(8,2)) as avg_MonthlyMinutes, \
            CAST (avg(CurrentEquipmentDays) as decimal(8,2)) as avg_CurrentEquipmentDays, \
            CAST (avg(TotalRecurringCharge) as decimal(8,2)) as avg_TotalRecurringCharge, \
            CAST (avg(OverageMinutes) as decimal(8,2)) as avg_OverageMinutes, \
            CAST (avg(RoamingCalls) as decimal(8,2)) as avg_RoamingCalls \
            FROM df GROUP BY Churn"
).show()


# Check the values of string columns
df.select([column for column in df.columns if column in columns_string]).show(1)



# Loop the list and use StringIndexer encodes the string columns of labels(Yes or No) to columns of label indices(1 or 0)
for column in columns_for_indexer:
    indexer = StringIndexer(inputCol=column, outputCol=column + "Index")
    df = indexer.fit(df).transform(df)





# Drop the original string columns of labels
df = df.select([column for column in df.columns if column not in columns_for_indexer])



# Check the Churn column again to confirm the result
df.select("ChurnIndex").describe().show()




# Replace unknown values to 0 for Price Column
df = df.replace(to_replace={"Unknown": "0"}, subset=["HandsetPrice"])




# Delete the rows with 'NA' values
for column in df.columns:
    if df.schema[column].dataType == pyspark.sql.types.StringType():
        df = df.filter(df[column] != "Unknown")





# Another Iteration for feature engineering
columns_string = [
    column
    for column in df.columns
    if df.schema[column].dataType == pyspark.sql.types.StringType()
]
columns_string




# Mapping string values into integer based on its values
mapping_PrizmCode = {"Other": "0", "Suburban": "1", "Town": "2", "Rural": "3"}
df = df.replace(to_replace=mapping_PrizmCode, subset=["PrizmCode"])






# Prepare the list of columns to be transformed into float
columns_to_float = [
    "MonthlyRevenue",
    "MonthlyMinutes",
    "TotalRecurringCharge",
    "DirectorAssistedCalls",
    "OverageMinutes",
    "RoamingCalls",
    "PercChangeMinutes",
    "PercChangeRevenues",
]




# Transform ServiceArea and CreditRating columns only to keep the int value
df = df.withColumn("ServiceArea", df["ServiceArea"].substr(-3, 3))
df = df.withColumn("CreditRating", df["CreditRating"].substr(1, 1))




# Transform the type of columns to float
for column in columns_to_float:
    df = df.withColumn(column, df[column].cast("float"))



# Create a Pandas DataFrame for data visualization
df_pd = df.toPandas()





# Before transform the type of Occupation column, let's check it's distribution first
graph = df_pd[["Occupation", "MonthlyRevenue"]]
ax = sns.barplot(x="Occupation", y="MonthlyRevenue", data=graph)




# Transform string values to numbers using mapping
temp = (
    df_pd.loc[:, ["Occupation", "MonthlyRevenue"]]
    .groupby("Occupation")
    .mean()
    .sort_values(["MonthlyRevenue"], ascending=[0])
)

mapping_Occupation = dict([temp.index[i], str(i)] for i in range(len(temp)))
print(mapping_Occupation)

df = df.replace(to_replace=mapping_Occupation, subset=["Occupation"])




# Prepare the list of columns to be transformed into integer
columns_to_int = [
    "Handsets",
    "HandsetModels",
    "CurrentEquipmentDays",
    "AgeHH1",
    "AgeHH2",
    "HandsetPrice",
    "ServiceArea",
    "CreditRating",
    "PrizmCode",
    "Occupation",
    "ChurnIndex",
    "ChildrenInHHIndex",
    "HandsetRefurbishedIndex",
    "HandsetWebCapableIndex",
    "TruckOwnerIndex",
    "RVOwnerIndex",
    "HomeownershipIndex",
    "BuysViaMailOrderIndex",
    "RespondsToMailOffersIndex",
    "OptOutMailingsIndex",
    "NonUSTravelIndex",
    "OwnsComputerIndex",
    "HasCreditCardIndex",
    "NewCellphoneUserIndex",
    "NotNewCellphoneUserIndex",
    "OwnsMotorcycleIndex",
    "MadeCallToRetentionTeamIndex",
    "MaritalStatusIndex",
]




# Transform the type of columns to integer
for column in columns_to_int:
    df = df.withColumn(column, df[column].cast("int"))

# Validation for hyper-parameter tuning.
# Randomly splits the input dataset into train and validation sets,
# and uses evaluation metric on the validation set to select the best model.
rf = RandomForestClassifier(
    featuresCol="features", labelCol="label", predictionCol="prediction", maxBins=16
)

# Use a ParamGridBuilder to construct a grid of parameters to search over.
# TrainValidationSplit will try all combinations of values and determine best model using the evaluator.
paramGrid = (
    ParamGridBuilder()
    .addGrid(rf.maxDepth, [8, 10, 12])
    .addGrid(rf.minInstancesPerNode, [1, 3, 5, 10])
    .addGrid(rf.numTrees, [50, 100])
    .build()
)

# In this case the estimator is BinaryClassificationEvaluator
# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.
tvs = TrainValidationSplit(
    estimator=rf,
    estimatorParamMaps=paramGrid,
    evaluator=BinaryClassificationEvaluator(),
    trainRatio=0.8,
)  # 80% of the data will be used for training, 20% for validation.

# Run TrainValidationSplit, and choose the best set of parameters.
model = tvs.fit(train)

Trong đoạn mã trên, chúng ta đang thực hiện quá trình tinh chỉnh siêu tham số (hyper-parameter tuning) cho mô hình Random Forest Classifier bằng cách sử dụng kỹ thuật Train-Validation Split. Dưới đây là giải thích chi tiết về các tham số được sử dụng trong quá trình tinh chỉnh:

RandomForestClassifier: Là một mô hình phân loại dựa trên cây quyết định, trong đó các cây quyết định được sử dụng để xây dựng một tập hợp các cây ngẫu nhiên, và kết quả cuối cùng là sự bỏ phiếu của các cây đó.

featuresCol: Là tên cột chứa các đặc trưng (features) trong dữ liệu.

labelCol: Là tên cột chứa nhãn (labels) trong dữ liệu, đây là nhãn dự đoán.

predictionCol: Là tên cột trong DataFrame kết quả được dự đoán.

maxBins: Là số lượng tối đa của các bin khi chia các đặc trưng liên tục thành các bin. Đây là một siêu tham số của Random Forest.

ParamGridBuilder: Là một builder để tạo ra một lưới các tham số để tinh chỉnh. Trong đoạn mã này, chúng ta tạo một lưới các giá trị cho các tham số của mô hình Random Forest như maxDepth, minInstancesPerNode, và numTrees.

maxDepth: Là độ sâu tối đa của các cây quyết định trong mô hình Random Forest.

minInstancesPerNode: Số lượng mẫu tối thiểu cần có trong một nút lá của cây quyết định.

numTrees: Số lượng cây quyết định trong mô hình Random Forest.

TrainValidationSplit: Là một kỹ thuật tinh chỉnh siêu tham số trong đó tập dữ liệu được chia thành tập huấn luyện và tập validation. Các tham số mô hình được đánh giá dựa trên hiệu suất trên tập validation.

estimator: Mô hình cần được tinh chỉnh, trong trường hợp này là mô hình Random Forest Classifier.

estimatorParamMaps: Là một lưới các tham số cần được đánh giá.

evaluator: Bộ đánh giá sẽ được sử dụng để đánh giá hiệu suất của mô hình trên tập validation. Trong đoạn mã này, chúng ta sử dụng BinaryClassificationEvaluator để đánh giá mô hình dựa trên đánh giá về hiệu suất phân loại nhị phân.

trainRatio: Tỷ lệ dữ liệu sẽ được sử dụng cho việc huấn luyện, phần còn lại sẽ được sử dụng cho việc validation. Trong đoạn mã này, chúng ta sử dụng tỷ lệ 80-20, tức là 80% dữ liệu sẽ được sử dụng cho huấn luyện và 20% còn lại sẽ được sử dụng cho validation.

fit(train): Thực hiện quá trình tinh chỉnh siêu tham số bằng cách sử dụng Train-Validation Split trên tập dữ liệu huấn luyện (train). Kết quả là một mô hình tốt nhất được chọn dựa trên hiệu suất trên tập validation.



# Validation for hyper-parameter tuning.
# Randomly splits the input dataset into train and validation sets,
# and uses evaluation metric on the validation set to select the best model.
gbt = GBTClassifier(
    featuresCol="features", labelCol="label", predictionCol="prediction"
)

# Use a ParamGridBuilder to construct a grid of parameters to search over.
# TrainValidationSplit will try all combinations of values and determine best model using the evaluator.
paramGrid = (
    ParamGridBuilder()
    .addGrid(gbt.maxDepth, [8, 10])
    .addGrid(gbt.minInstancesPerNode, [5, 20, 50])
    .addGrid(gbt.maxIter, [10, 20])
    .build()
)

# In this case the estimator is BinaryClassificationEvaluator
# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.
tvs = TrainValidationSplit(
    estimator=gbt,
    estimatorParamMaps=paramGrid,
    evaluator=BinaryClassificationEvaluator(),
    trainRatio=0.8,
)  # 80% of the data will be used for training, 20% for validation.

# Run TrainValidationSplit, and choose the best set of parameters.
model = tvs.fit(train)

# Make predictions on test data. model is the model with combination of parameters that performed best.
model.transform(test).select("features", "label", "prediction").show(10)


Trong đoạn mã trên, chúng ta đang thực hiện quá trình tinh chỉnh siêu tham số (hyper-parameter tuning) cho mô hình Gradient Boosted Tree (GBT) Classifier bằng cách sử dụng kỹ thuật Train-Validation Split. Dưới đây là giải thích chi tiết về các tham số được sử dụng trong quá trình tinh chỉnh:

GBTClassifier: Là một mô hình phân loại dựa trên cây quyết định, trong đó các cây quyết định được sử dụng để xây dựng một tập hợp các cây ngẫu nhiên, và kết quả cuối cùng là sự bỏ phiếu của các cây đó.

featuresCol: Là tên cột chứa các đặc trưng (features) trong dữ liệu.

labelCol: Là tên cột chứa nhãn (labels) trong dữ liệu, đây là nhãn dự đoán.

predictionCol: Là tên cột trong DataFrame kết quả được dự đoán.

ParamGridBuilder: Là một builder để tạo ra một lưới các tham số để tinh chỉnh. Trong đoạn mã này, chúng ta tạo một lưới các giá trị cho các tham số của mô hình GBT như maxDepth, minInstancesPerNode, và maxIter.

maxDepth: Là độ sâu tối đa của các cây quyết định trong mô hình GBT.

minInstancesPerNode: Số lượng mẫu tối thiểu cần có trong một nút lá của cây quyết định.

maxIter: Số lượng lần lặp tối đa khi huấn luyện mô hình.

TrainValidationSplit: Là một kỹ thuật tinh chỉnh siêu tham số trong đó tập dữ liệu được chia thành tập huấn luyện và tập validation. Các tham số mô hình được đánh giá dựa trên hiệu suất trên tập validation.

estimator: Mô hình cần được tinh chỉnh, trong trường hợp này là mô hình GBT Classifier.

estimatorParamMaps: Là một lưới các tham số cần được đánh giá.

evaluator: Bộ đánh giá sẽ được sử dụng để đánh giá hiệu suất của mô hình trên tập validation. Trong đoạn mã này, chúng ta sử dụng BinaryClassificationEvaluator để đánh giá mô hình dựa trên đánh giá về hiệu suất phân loại nhị phân.

trainRatio: Tỷ lệ dữ liệu sẽ được sử dụng cho việc huấn luyện, phần còn lại sẽ được sử dụng cho việc validation. Trong đoạn mã này, chúng ta sử dụng tỷ lệ 80-20, tức là 80% dữ liệu sẽ được sử dụng cho huấn luyện và 20% còn lại sẽ được sử dụng cho validation.

fit(train): Thực hiện quá trình tinh chỉnh siêu tham số bằng cách sử dụng Train-Validation Split trên tập dữ liệu huấn luyện (train). Kết quả là một mô hình tốt nhất được chọn dựa trên hiệu suất trên tập validation.
